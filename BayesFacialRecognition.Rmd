---
title: "Bayesian Modelling of Cognitive Processes Group Project"
author: "Anita Vrins, Ronja Rönnback, Louis Sillekens"
date: "2023-11-18"
output: html_document
---

## Facial Recognition of Public Figures

In this rmarkdown document the code of our project is located. Some comments exist, but for a better explanation, please read our separate report.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# set the working directory here for the entire notebook
#knitr::opts_knit$set(root.dir = ("/Users/ronjaronnback/Documents/GitHub/BayesianFaceRecognition"))
#knitr::opts_knit$set(root.dir = '("C:/Users/louis/Documents/University/Master (TiU)/Year 2/Courses/BayesModels/FacialRec/BayesianFaceRecognition')
```

```{r imports, include=FALSE, echo=FALSE}
# Loading packages --------------------------------------------------------

library(tidyr)
library(dplyr)
options(mc.cores = parallel::detectCores())
library(bcogsci)
library(extraDistr)
library(bayesplot)
library(ggplot2)
library(rstan)
library(posterior)
rstan_options(auto_write = FALSE)

```

### Load the data

We load the data and do some general preprocessing, as well as declaring the functions for each tree outcome:

* NR -- 1 - "I did not recognise the person"
* C --- 2 - "I got it right"                            
* TT -- 3 - "I got it wrong, but the correct name was “on the tip of my tongue”"
* RNN - 4 - "I recognised the person, but I could not remember their name"

```{r load data, message=FALSE}
# Load Data ---------------------------------------------------------------
data <-read.csv("data/outcome_data.csv")

# define min-max scaling function
min_max_scale <- function(x){(x-min(x))/(max(x) - min(x))}

# min-max scale and center data on 0
data$C_TotalPageViews <- min_max_scale(data$TotalPageViews) - 0.5

# Find probabilities for each branch --------------------------------------
value_counts <- table(data$outcome)
value_counts 

p_true <- .58
q_true <- .66
r_true <- .23

# Declaring Functions ---------------------------------------------------------

NotRecognised <-function(p,q,r) # NR
  1 - p

RecognisedAndNamed <-function(p,q,r) #C
  p * q

TipOfTongue <- function(p,q,r) #TT
  p * (1 - q) * r

RecognisedNotNamed <- function(p,q,r) #RNN
  p * (1 - q) * (1 - r)
```

### Base Multinomial Processing Tree Model 

First, we examine the simple MPT model, on synthetic data:
```{r preprocessing, message=FALSE}
# Creating a Dataframe -------------------------------------------------

theta_NR <- NotRecognised(p_true, q_true, r_true)
theta_C <- RecognisedAndNamed(p_true, q_true, r_true)
theta_TT <- TipOfTongue(p_true, q_true, r_true)
theta_RNN <- RecognisedNotNamed(p_true, q_true, r_true)

# generate data vector of probabilities
Theta <- tibble(theta_NR,
    theta_C,
    theta_TT,
    theta_RNN)

# generate values of a multinomial distribution of responses given Theta
N_trials <- 200
ans <- rmultinom(1, N_trials, c(Theta))
```
#### Fit Base MPT Model
```{r base model, message=TRUE}
# BASE Stan Model --------------------------------------------------------------

data_face <-  list(N_trials = N_trials,
                   ans = c(ans)) 

fit_face <- stan("Facial.stan", data = data_face)


# Model Inspection --------------------------------------------------------
print(fit_face, pars = c("p", "q", "r"))


as.data.frame(fit_face) %>%
  select(c("p","q","r")) %>%
  mcmc_recover_hist(true = c(p_true, q_true, r_true)) +
  coord_cartesian(xlim = c(0, 1))

print(fit_face, pars = c("theta")) # nice & close to the derived "true" values!
```

```{r remove base, message=FALSE, echo=FALSE}
# remove model because otherwise Ronja's laptop crashes
remove(fit_face)
```

### Hierarchical Multinomial Processing Tree Model ON SIMULATED DATA

What follows is the hierarchical MPT model, fitted on simulated data to test for parameter recovery. True values are defined through estimates, but we generally leave it quite open (for example, we take no firm stance to how strong the beta's should be, but we expect them to be positive).

```{r simulated data preparation, message=FALSE}
N_item <- 20 # number trials per subject
N_subj <- 176 # number of subjects
N_obs <- N_item * N_subj 

subj <- rep(1:N_subj, each = N_item)
trial_number <- rep(1:N_item, time = N_subj)
# make approximate distribution for complexity (fame) and center it
complexity <- min_max_scale(rep(rlnorm(N_item, meanlog = 0, sdlog = 1), 
                                      times = N_subj)) - 0.5

# define simulated true parameters
p_true <- 0.58
q_true <- 0.66
r_true <- 0.23

tau_u <- c(1.1,1.1)
u <- matrix(nrow = N_subj,ncol = 2)
u[,1:2] <- c(rnorm(N_subj, 0, tau_u[1]), rnorm(N_subj, 0, tau_u[2]))

alpha_p <- 0
beta_p <- 1
p_true <- plogis(alpha_p + u[subj,1] + complexity * beta_p)

alpha_q <- 0
beta_q <- 1
q_true <- plogis(alpha_q + u[subj,2] + complexity * beta_q)

# generate data vector of probabilities
theta_h <- matrix(
  c(NotRecognised(p_true, q_true, r_true),
    RecognisedAndNamed(p_true, q_true, r_true),
    TipOfTongue(p_true, q_true, r_true),
    RecognisedNotNamed(p_true, q_true, r_true)),
  ncol = 4)

# generate values of a categorical distribution of responses given Theta
ans <- rcat(N_obs,theta_h)

# make tibble of our real data
sim_exp <- tibble(subj = subj,
                   item = trial_number,
                   complexity = complexity,
                   w_ans = ans)

# make list of our experiment data
sim_exp_list <-  list(onlyprior = 0,
                      N_obs = nrow(sim_exp),
                      w_ans = sim_exp$w_ans,
                      N_subj = max(sim_exp$subj),
                      subj = sim_exp$subj,
                      complexity = sim_exp$complexity)
```
#### Fit model

We now fit the model, this may take some time:
```{r sim hierarchical model fit, message=FALSE}
# get STAN model
mpt_hierarch_sim <- stan("HFR_Improved.stan", data = sim_exp_list)
```

Let's see the output:

```{r sim hierarchical model output, message=FALSE}
print(mpt_hierarch_sim,
      pars = c("r", "tau_u", "alpha_p", "beta_p", "alpha_q", "beta_q"))
# parameter recovery pretty good!

# see if we converged: Looks good!
traceplot(mpt_hierarch_sim) +
  ggtitle("Traceplot of Hierarchical Model on Simulated Data") + 
  theme(plot.title = element_text(size = 12))

# posterior predictive checks for simulated data
as.data.frame(mpt_hierarch_sim) %>%
  select(r, alpha_p, beta_p, alpha_q, beta_q) %>%
  mcmc_recover_hist(true = c(r_true, alpha_p, beta_p, alpha_q, beta_q)) +
  ggtitle("Posterior Distributions of the Hierarchical Model \non Simulated Data") + 
  theme(plot.title = element_text(size = 12))
```

It looks like we converge, and recover the parameters quite well.

```{r sim hierarchical model remove, message=FALSE, echo=FALSE}
# remove model because otherwise Ronja's laptop crashes
remove(mpt_hierarch_sim)
remove(sim_exp_list)
remove(sim_exp)
```


### Hierarchical Multinomial Processing Tree Model ON REAL DATA

In the following section we proceed to fit the model on the actual data. 

```{r hierarchical model, message=FALSE}
# HIERARCHICAL Stan Model --------------------------------------------------

N_item <- 20 # number trials per subject
N_subj <- 176 # number of subjects
N_obs <- N_item * N_subj 

# make tibble of our real data
exp <- tibble(subj = data$participant,
               item = data$trial_number,
               complexity = data$C_TotalPageViews,
               w_ans = data$outcome)

# make list of our experiment data
exp_list_h <-  list(onlyprior = 0,
                    N_obs = nrow(exp),
                    w_ans = exp$w_ans,
                    N_subj = max(exp$subj),
                    subj = exp$subj,
                    complexity = exp$complexity)
# get STAN model
mpt_hierarch <- stan("HFR_Improved.stan", data = exp_list_h)
```

Let's see the output:

```{r hierarchical model output, message=FALSE}
print(mpt_hierarch, pars = c("alpha_p", "beta_p", "alpha_q", "beta_q", "r", "tau_u"))

# see if we converged:
traceplot(mpt_hierarch) +
  ggtitle("Traceplot of Hierarchical Model on Actual Data") +
  theme(plot.title = element_text(size = 12))

```

We see that the chains converge, luckily, and the Rhat values are all quite acceptable. Our r-value is verly close or identical to what was estimated from previous calculations of the tree. The regression slopes using fame (through total Wikipedia page views) are all strongly positive, indicating a strong positive influence of fame on the branches - as expected, the more famous an individual is, the higher the chance that they are recognised (branch p) and correctly named (branch q).

### Posterior Predictive Checks of Hierarchical Model

We here plot various posterior predictive checks of our model. As a brief reminder, the outcome options are coded as follows:

* NR -- 1 - "I did not recognise the person"
* C --- 2 - "I got it right"                            
* TT -- 3 - "I got it wrong, but the correct name was “on the tip of my tongue”"
* RNN - 4 - "I recognised the person, but I could not remember their name"
 
```{r posterior, message=FALSE}

mcmc_hist(mpt_hierarch, pars = c("r", "alpha_p", "beta_p", "alpha_q", "beta_q")) +
  ggtitle("Posterior Distribution of the Hierarchical Model") + 
  theme(plot.title = element_text(size = 12))

# bar plot as posterior predictive check
gen_data <- rstan::extract(mpt_hierarch)$pred_w_ans
ppc_bars(exp$w_ans, gen_data) +
  ggtitle ("Posterior Predictive Distribution of the Hierarchical Model")  + 
  theme(plot.title = element_text(size = 12))

```

Concerning the posterior distributions, we observe quite large beta's for both p (not recognized versus recognized branch) and q (correctly named versus recognized but not correctly named branch) - as mentioned, this is as expected, since we assumed that fame would strongly positively influence both recognition and naming.

Regarding the posterior predictive distribution, we also see that the model generally fits the outcomes quite splendidly!

Moving on, we visualize how the model fares not globally, but for individual subjects:

```{r posterior per subj, message=FALSE}

# get subset of 12 subjects (for pretty & readable graph)
temp <- head(unique(exp$subj), 12)
exp_subset <- exp[exp$subj%in%temp,]

# get posterior predictions for these 12 subjects only
ppc_bars_grouped(exp_subset$w_ans, 
                 gen_data[,1:240], group = exp_subset$subj) +
  ggtitle ("By-subject plot for the hierarchical model") + 
  theme(plot.title = element_text(size = 12))


```

Now, we plot the posterior predictive distributions for four different tiers of fame in the dataset: either those individuals with the highest total page views (A-tier), until those with the least total page views (D-tier). The breaks are equidistant.

```{r posterior fame, message=FALSE}

# MAKE POSTERIOR CHECK FOR FAMOUS OR NOT FAMOUS PEOPLE
fame_tiers <- cut(data$C_TotalPageViews, breaks = 4, 
                  labels = c("D-Tier", "C-Tier", "B-Tier", "A-Tier"), include.lowest = TRUE)

# want frequency, not counts!
ppc_bars_grouped(exp$w_ans, gen_data, group = fame_tiers, freq = FALSE) + 
  ggtitle ("Posterior predictive distributions for different tiers\nof fame") + 
  theme(plot.title = element_text(size = 12))


```

As we see, the posterior predictive distributions for the A & B-tier group are showing a significantly higher proportion of correctly named items (outcome 2 on the x-axis). This is in line with expectations - the more famous the person presented, the more likely it is that they'll be recognized and correctly named. Generally, the model does quite a good job, and the lower the tier, the lower the proportion of outcomes 2.

Let's plot the posterior predictive distributions for the most famous individual in the dataset (Michael Jackson, with total page views being the maximum possible, 1), a "mid-tier celebrity", in this case Bob Dylan, with a total page views of 0.2. The posterior predictive distribution for all other celebrities is visualized for the sake of comparison.

```{r posterior celebrity comparison, message=FALSE}

# MAKE POSTERIOR CHECK FOR FAMOUS OR NOT FAMOUS PEOPLE
MJNAorNOT <- factor(ifelse(data$name=="Michael Jackson", "Michael Jackson", 
                         ifelse(data$name=="Bob Dylan", "Bob Dylan", 
                         ifelse(data$name=="Steve Jobs", "Steve Jobs", 
                         "Other Celebrities"))))

# want frequency, not counts!
ppc_bars_grouped(exp$w_ans, gen_data, group = MJNAorNOT, freq = FALSE) + 
  ggtitle ("Plot for the hierarchical model for Michael Jackson, Bob Dylan or\nother celebrities") + 
  theme(plot.title = element_text(size = 12))


```




